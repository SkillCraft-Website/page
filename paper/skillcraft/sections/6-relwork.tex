\section{Related Work}
% Existing tool-use benchmarks can be positioned along two main axes: whether the tools are genuinely executable with environment states close to production systems, and whether the tasks require long-horizon planning with composition across subtasks. On the more controlled end, BFCL~\citep{patilberkeley} abstracts tool invocation into structured function-parameter prediction, which makes it easy to compare models on calling format and tool selection. $\tau$-Bench~\citep{yao2024tau} and ACEBench~\citep{chen2025acebench} similarly emphasize multi-turn interaction and correctness of tool choice, typically using controlled settings to ensure reproducibility. Gorilla~\citep{patil2023gorilla} and AgentBench~\citep{liu2023agentbench} also primarily test whether an agent can pick the right API or tool, offering broad coverage across tools and domains. However, these benchmarks rarely force agents to perform programmatic abstraction of recurring structures, because many tasks can be solved with one-shot selection or short tool-call chains. ToolLLM~\citep{qin2023toolllm} further studies tool retrieval and model fine-tuning, but likewise focuses on single-use accuracy rather than cross-task reuse of patterns. For SkillCraft, , these works are useful baselines for whether a call is correct, but they provide weak signal for whether an agent consolidates recurring call patterns into reusable procedures, which is essential for explaining why some agents only succeed once while others become increasingly efficient on structurally similar subtasks.

% Moving closer to realistic execution, a growing set of benchmarks places agents in executable tools or high-fidelity application environments. AppWorld~\citep{trivedi2024appworld} supports richer interaction and state transitions through application-level environments. The MCP ecosystem (including MCPWorld~\citep{yan2025mcpworld}, MCP-RADAR~\citep{gao2025mcp}, MCPEval~\citep{liu2025mcpeval}, MCP-AgentBench~\citep{guo2025mcp}, LiveMCPBench~\citep{mo2025livemcpbench}, MCPAtlas~\citep{bandimcp}, etc.) standardizes tool integration and enables more systematic evaluation across servers and applications, but many tasks remain single-application and often rely on simplified or manually designed initial states. WebArena~\citep{zhou2023webarena}, OSWorld~\citep{xie2024osworld}, SWE-Bench~\citep{jimenez2024swebench} and TheAgentCompany~\citep{xu2024theagentcompanybenchmarkingllmagents} shift the emphasis toward long-horizon execution and error recovery in real web, desktop, and codebase workflows. GAIA~\citep{mialon2023gaia}, ARE~\citep{froger2025scaling}, and BrowseComp~\citep{wei2025browsecomp} focus on broad competence in daily tasks or browsing-based retrieval. Tool Decathlon (Toolathlon)~\citep{li2025tool} is pivotal on this trajectory: it combines real tools, fuzzy instructions, execution-based verification, and long-horizon cross-application workflows within one framework, setting a strong realism-oriented reference point. However, most benchmarks remain centered on the question of whether an agent can finish a task given atomic tools, meaning that even long tasks can often be solved by planning from scratch and incrementally invoking atomic tools. SkillCraft complements this landscape by organizing tasks into families with recurring substructure and scalable difficulty, so that skill accumulation becomes an observable and comparable capability rather than an incidental byproduct of long trajectories.

% On the agent pipeline side, mainstream tool-calling systems largely inherit the “reasoning–acting–observing" loop paradigm established by ReAct~\citep{yao2023react}, which interweaves natural language reasoning with external tool invocation, forming the foundational skeleton of today's tool-using agents. In parallel, another line of work treats programs or skills as the basic unit of action organization. CodeAct~\citep{wang2024executable} uses executable code to express control flow and orchestrate multi-tool interactions, reducing dependence on long natural-language chains, but it still requires generating task-specific code from scratch and does not directly support accumulation of reusable patterns across tasks. Voyager~\citep{wang2023voyager}, Ghost in the Minecraft~\citep{zhu2023ghost} demonstrate that agents can build up a library of reusable code skills through autonomous exploration and curriculum learning, but the portability of those skills is bounded by the closed rules and state spaces of game environments. CREATOR~\citep{qian2023creator} and related methods attempt to abstract reusable components from recurring patterns, yet they typically provide limited systematic evidence of cross-task generalization in realistic tool ecosystems. In more traditional planning paradigms, Hierarchical Task Networks (HTN)~\citep{georgievski2014overview} emphasize recursive decomposition of complex problems into reusable sub-skills that support compositional generalization. While effective in robotics, these approaches become challenging when adapted to LLM agents because debugging, credit assignment, and error propagation can quickly dominate as hierarchies deepen. Anthropic Skills~\citep{anthropic_agentskills} provides an engineering-oriented mechanism for packaging reusable workflows as explicit skill modules, but these modules are primarily authored and configured by humans rather than autonomously discovered by an agent at test time. SkillCraft targets this gap by providing a minimal MCP protocol with save, execute, list, and get primitives that lets agents automatically abstract successful tool sequences into verified, executable skills. By evaluating cross-task reuse, cross-difficulty transfer, and cross-model transfer, SkillCraft turns efficiency gains and skill generalization into quantifiable dimensions that can be compared across agent designs and tool-use settings.


Tool-use benchmarks mainly differ in the realism of tool executability and in whether tasks require long-horizon composition. In controlled settings, BFCL~\citep{patilberkeley} reduces tool use to structured function-parameter prediction, while $\tau$-Bench and ACEBench emphasize multi-turn interaction and correct tool selection under reproducible environments~\citep{yao2024tau, chen2025acebench}. Gorilla and AgentBench broaden tool and domain coverage~\citep{patil2023gorilla, liu2023agentbench}, but primarily evaluate API selection, such that short tool-call chains often suffice.

% ToolLLM~\citep{qin2023toolllm} studies tool retrieval and fine-tuning, yet its signal is still dominated by single-use accuracy rather than reuse of recurring patterns. 
More realistic benchmarks execute tools in richer environments. AppWorld supports application-level state transitions~\citep{trivedi2024appworld}, and MCP-based suites such as MCPWorld, MCP-RADAR, MCPEval, MCP-AgentBench, LiveMCPBench, and MCPAtlas standardize tool integration across servers~\citep{yan2025mcpworld, gao2025mcp, liu2025mcpeval, guo2025mcp, mo2025livemcpbench, bandimcp}, though tasks often remain single-application with simplified initial states. WebArena, OSWorld, SWE-Bench, and TheAgentCompany emphasize long-horizon execution and error recovery in web, desktop, and code workflows~\citep{zhou2023webarena, xie2024osworld, jimenez2024swebench, xu2024theagentcompanybenchmarkingllmagents}, while GAIA, ARE, and BrowseComp focus on everyday tasks and web-based information seeking~\citep{mialon2023gaia, froger2025scaling, wei2025browsecomp}. Tool Decathlon (Toolathlon) further consolidates real tools, fuzzy instructions, execution verification, and cross-application workflows~\citep{li2025tool}.
% SkillCraft complements this line by structuring tasks as families with recurring structure and scalable difficulty to make procedure reuse measurable.

On the pipeline side, most tool-using agents follow the “reasoning–acting–observing" loop introduced by ReAct~\citep{yao2023react}, where planning and state tracking are repeated at every tool call. CodeAct~\citep{wang2024executable} shifts the action space to executable code to express control flow and multi-tool orchestration, but it still regenerates code per task and does not accumulate reusable procedures. Voyager~\citep{wang2023voyager} and Ghost in the Minecraft~\citep{zhu2023ghost} show that agents can grow a code skill library through exploration, yet the resulting skills are tied to game rules and state spaces. CREATOR~\citep{qian2023creator} abstracts reusable components from patterns but provides limited evidence of robust cross-task generalization in realistic tool ecosystems. 
% Hierarchical Task Networks~\citep{georgievski2014overview} formalize recursive decomposition into sub-skills, but when adapted to LLM agents they face cascading errors and high debugging overhead. 
Anthropic Skills~\citep{anthropic_agentskills} packages workflows as explicit skill modules, but these modules are typically authored and configured by humans rather than induced from execution. In contrast, SkillCraft enables autonomous reuse with a minimal MCP protocol that compiles successful tool sequences into verified executable skills.